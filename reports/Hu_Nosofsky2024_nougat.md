# High-variability training does not enhance generalization in the prototype-distortion paradigm

Mingjia Hu

Robert M. Nosofsky

###### Abstract

Classic studies of human categorization learning provided evidence that high-variability training in the prototype-distortion paradigm enhances subsequent generalization to novel test patterns from the learned categories. More recent work suggests, however, that when the number of training trials is equated across low-variability and high-variability training conditions, it is low-variability training that yields better generalization performance. Whereas the recent studies used cartoon-animal stimuli varying along binary-valued dimensions, in the present work we return to the use of prototype-distorted dot-pattern stimuli that had been used in the original classic studies. In accord with the recent findings, we observe that high-variability training does not enhance generalization in the dot-pattern prototype-distortion paradigm when the total number of training trials is equated across the conditions, even when training with very large numbers of distinct instances. A baseline version of an exemplar model captures the major qualitative pattern of results in the experiment, as do prototype models that make allowance for changes in parameter settings across the different training conditions. Based on the modeling results, we hypothesize that although high-variability training does not enhance generalization in the prototype-distortion paradigm, it may do so when participants learn more complex category structures.

Categorical Generalization Training-instance variability Computational modeling 2020

## Introduction

An issue of fundamental theoretical and practical significance in the domain of categorization is how to arrange learning conditions to promote successful generalization to novel members of the learned categories. There is a vast literature that examines the influence of multiple variables on varieties of category generalization. One of the major such variables is training-instance variability (e.g., Bowman & Zeithamova, 2023; Cohen, Nosofsky, & Zaki, 2001; Hahn, Bailey, & Elvin, 2005; Hintzman, 1986; Homa & Cultice, 1984; Homa & Vosburgh, 1976; Minda & Smith, 2001; Peterson et al., 1973; Posner & Keele, 1968; Stewart & Chater, 2002). In the present article our focus is on the influence of training-instance variability in promoting successful generalization in cases involving prototype-based categories. These are categories defined around central prototypes in which training instances are constructed by distorting the prototypes using statistical-distortion procedures. The question is whether it is better to learn such categories by being trained on low-variability instances that closely resemble the prototypes or on higher-variability instances that are not highly similar to the prototypes.

Early classic articles in the human categorization literature suggested that although initial learning is fostered by training with low-variability instances, subsequent generalization to novel high-distortion category members is fostered if observers are trained with higher-variability instances. Much of this evidence came from the seminal "dot-pattern prototype-distortion" paradigm developed by Posner and Keele (1968, 1970) and pursued in comprehensive fashion by Homa and his colleagues (e.g., Homa, Blair, et al., 2019; Homa & Cultice, 1984; Homa, Sterling, & Trepel, 1981; Homa & Vosburgh, 1976). In typical versions of this paradigm, category prototypes are defined by placing dots in random locations of a grid and sometimes connecting the dots sequentially with lines to form polygons. Training instances are then constructed by using a statistical-distortion algorithm on those prototypes. Low distortions that are highly similar to the prototype areconstructed by displacing the individual-dot locations by small magnitudes in random directions; higher-level distortions are constructed by displacing the dot locations by larger magnitudes. Posner and Keele (1968) found that generalization to novel high-distortion instances was fostered if participants were trained using high-distortion instances, and they suggested that participants learned about both the prototypes and the variability of the categories. Homa and Vosburgh (1976) jointly manipulated the variability of the instances and the size of the to-be-learned categories, where category size is defined as the number of training instances in each category. They found that generalization to novel instances of the larger-size categories was fostered with higher-variability training.

The general take-home message from these classic studies was summarized by Homa, Sterling, and Trepel (1981, p. 420): "...when the within-category stimulus variance is increased for categories defined by numerous exemplars (Homa and Vosburgh, 1976), generalization to new instances is improved." This general take-home message has survived to the present day (e.g., Doyle and Hourihan, 2016; Raviv et al., 2022). For example, Raviv et al. (2022) presented a comprehensive review of how different forms of variability influence wide varieties of learning and generalization. Included in their review was how variability of training instances influences generalization in the prototype-distortion paradigm. One of their primary lead-off examples (see Raviv et al., 2022, Fig. 1) was the finding from Posner and Keele (1968) that, although initial learning was more difficult, generalization was enhanced when participants were trained with high-variability instances.

However, in influential work that introduced a form of exemplar-based modeling of category learning and representation, Hintzman (1984, 1986) pointed to an important limitation of these early studies: participants had been trained to a performance criterion prior to their generalization performance being tested in a transfer phase. The training phase tended to take longer for participants trained on higher-variability instances. Thus, the length of the training phase tended to be confounded with the manipulation of the factor of training-instance variability. Hintzman (1984, 1986) noted that his exemplar-based MINERVA-2 model predicted that if the amount of training were to be equated, low-variability training sets should lead to better generalization than high-variability sets in the prototype-distortion paradigm.

Bowman and Zeithamova (2020, 2023a) have pursued this issue in recent studies using an alternative version of the prototype-distortion paradigm (see also Minda and Smith, 2001). Rather than using dot-pattern stimuli, Bowman and Zeithamova used cartoon-animal stimuli defined along a set of separable, binary-valued dimensions. The prototype of Category A might have logical-value 1 along each of its dimensions, whereas the prototype of Category B might have logical-value 2. Low and high distortions of the prototypes are constructed by varying the number of dimensions in which the prototype value is switched. In accord with the type of prediction derived from Hintzman's (1984, 1986) modeling, under conditions in which the total amount of training was equated across conditions, Bowman and Zeithamova (2023a) found that generalization to novel transfer items was enhanced in training conditions using low-variability, low-distortion training instances compared to high-variability ones.1

Footnote 1: In work in progress, Bowman and Zeithamova (2023b) reported results from an experiment that equated total training trials while manipulating another form of training-instance variability. In this study, the authors used face stimuli that were blends of “parent” and “parent”.

Figure 1: Illustration of the types of patterns produced through use of the Posner et al. (1967) dot-pattern distortion procedure. The figure shows two examples each of the low, medium, and high distortions of a prototype

Although Bowman and Zeithamova's (2023a) evidence is intriguing, questions arise regarding the basis and generalizability of their findings. In particular, it is unclear to us the extent to which Bowman and Zeithamova's findings involving objects varying along a set of separable, binary-valued dimensions might generalize to cases involving more holistic objects embedded in continuous-dimension spaces. Our intuition is that, in the latter case, if an observer is tested with a high-distortion transfer pattern, performance would be enhanced if the observer had experienced a collection of high-distortion training items that were similar to that high-distortion test pattern. This form of generalization enhancement seems less likely to occur in cases in which the stimuli are composed of sets of separable binary-valued dimensions, because it is likely more difficult to form integrated memory representations of individual training items in such cases.

Thus, in the present work, we were motivated to revisit the seminal dot-pattern prototype-distortion paradigm to further investigate these issues, because the stimuli in this domain do seem to be both holistic in nature and embedded in continuous-dimension spaces. Our idea was to conduct manipulations similar to those reported in the classic studies of Posner and Keele (1968) and Homa and Vosburgh (1976), except, following Bowman and Zeithamova (2023a), train participants for a fixed number of training trials across low-variability and high-variability conditions.

Importantly, Homa and Cultice (1984) have in fact already reported such a study, although their findings are rarely discussed in relation to the present issues. Homa and Cultice's main interest was in comparing category learning under feedback versus no-feedback conditions in the dot-pattern prototype-distortion paradigm. In feedback tasks, participants are provided with knowledge of the correct category following each of their category guesses during the training phase; no such feedback is provided in the no-feedback condition. Homa and Cultice examined this issue across conditions in which both variability of training instances and category size were manipulated. Participants learned to classify prototype-distorted dot patterns into three categories. Category size was manipulated in a within-subject fashion, with the individual categories represented by three, six, or nine training instances. Category variability was manipulated in a between-subjects fashion, with participants trained on low-, medium-, high-, or mixed-level distortions of the prototypes. Crucially, the number of training trials was held fixed across the different variability conditions. Following training, the participants' generalization performance to novel members of the categories was tested. Homa and Cultice did not report analyses of the effects of the variability manipulation on generalization performance in the feedback condition, focusing instead on overall comparisons of performance across the feedback and no-feedback conditions, which was their main interest in this study. However, inspection of their Fig. 2 (top panel) makes clear that - in contrast to the pattern of results reported by Posner and Keele (1968) and Homa and Vosburgh (1976) - overall generalization performance was best in the low-variability training condition, intermediate in the medium condition, and worst in the high condition, with performance in the mixed-variability training condition being intermediate. Generalization to the novel high distortions was roughly the same across the conditions: there was certainly no advantage in classifying the novel high-distortion patterns for participants in the high- compared to the low-distortion training condition.

Our current experiment is similar to the feedback condition reported by Homa and Cultice (1984), but with several major changes in design that we describe below. Participants learned to classify prototype-distorted dot patterns into three prototype-based categories, and their generalization performance to novel dot patterns of varying degrees of distortion was then tested. We manipulated across conditions whether participants were trained using low-, medium-, high-, or mixed-level distortions of the prototypes. As in Bowman and Zeithamova (2023a) and Homa and Cultice (1984), the different variability conditions had the same number of training trials.

The major differences in the design of our study compared to the one of Homa and Cultice (1984) were as follows. First, recall that Homa and Cultice (1984) used category training-set sizes of three, six, and nine in which the same instances were presented repeatedly across the different blocks of training. By contrast, in our design, following recent studies reported by Homa, Blair, et al. (2019) and Hu and Nosofsky (2022), no single training instance was ever repeated. Instead, novel distortions of the prototypes were displayed on each and every training trial. As argued by Homa, Blair, et al. (see also Ashby and Maddox, 1992), when people learn numerous types of categories in the real world, such as birds, trees, faces, and so forth, it appears that only a tiny proportion of the training instances are ever exactly repeated. Hence, the non-repeating training paradigm seemed like a worthwhile one to test in the present situation. More important, given Homa and Vosburgh's (1976)finding that the benefits of high-variability training occurred mainly for large-size categories, the use of non-repeating sets of training exemplars would seem to be friendly to the high-variability hypothesis. Indeed, there is an important theoretical basis for testing the non-repeating paradigm. As described in more detail in the _Formal modeling_ section of our article, according to exemplar models, people classify novel test patterns on the basis of their similarity to training instances of the alternative categories (e.g., Hintzman, 1986; Medin & Schaffer, 1978; Nosofsky, 1986). If only a small set of high-distortion training patterns are presented during study, then they may occupy relatively sparse regions of the high-dimensional similarity spaces in which the present types of dot patterns are presumably embedded. Hence, in testing generalization to novel high-distortion test patterns, the use of high-distortion training may not be beneficial because the novel high-distortion test items may not be very similar to the old training instances. By using large sets of non-repeating training instances, we may increase the chances of having the high-distortion training instances cover the training space in a more complete fashion, thereby potentially promoting successful generalization under high-variability training conditions.

A second important difference between Homa and Cultice's (1984) paradigm and ours is that Homa and Cultice (1984) allowed participants to use a "None" response during the transfer tests: If participants believed that a test pattern did not belong to any of the three trained categories, they could respond "None." Although the use of "None" responses is a potentially important vehicle for investigating the detailed nature of people's category representations (e.g., see Hahn et al., 2005), we decided not to follow that procedure here, instead requiring participants to classify all test patterns into one of the three candidate categories. Our concern was that making use of the "None" response might lead to underestimates of the participants' category knowledge across the different experimental conditions, especially in testing the ability of participants to correctly

Figure 2: Mean proportion of correct classifications during the training phase as a function of training condition (low, medium, high, mixed) and training block. Error bars are one standard error of the mean

classify high-distortion test patterns under low-distortion training conditions. In particular, when presented with a high-distortion test pattern, a participant may realize that it is far less similar to the target category representation than are the low distortions with which they were trained. On this basis, they might respond "None" if that option were available. However, such participants might also realize that, relatively speaking, the high distortion is more similar to the category representation of its target category than to the contrast categories. Such information would allow them to correctly classify the novel high distortion. To avoid underestimation of this form of category knowledge, we decided not to allow "None" responses in the present experiment.

Finally, in Homa and Cultice (1984), the same three prototype patterns defined the alternative categories for all participants (see _Design_ section of Homa & Cultice, 1984, p. 86). By comparison, in our design, the three category prototypes were generated randomly anew for each and every participant. In our view, if one is seeking generality and robustness in identifying the phenomena of interest, then our random-generation procedure seems preferable. Homa and Cultice's design limits the inquiry to a restricted subset of the population of materials. If there were any idiosyncratic properties of the particular prototypes that were used, then the pattern of results that they observed may be limited in generality. We discuss some possibilities along these lines in subsequent sections of our article.

Before proceeding, we should re-emphasize that the present design deliberately introduces a type of confound into the testing situation (see also Bowman & Zeithamova, 2023). Because we are equating the total number of training trials across conditions, it will almost certainly be the case that, in the training phase itself, terminal learning performance will be worse in the high-variability condition than in the low-variability one. As reviewed by Raviv et al. (2022), such a pattern is observed ubiquitously across numerous learning paradigms that manipulate training-instance variability. The question is, despite this type of confound, might generalization to novel high-distortion instances _still_ be enhanced in the high-variability training conditions (Ravivi et al., 2022). To repeat, for numerous previous paradigms in the category-learning literature that addressed the variability question, it was the reverse confound that was involved: the amount of training in the high variability condition exceeded the amount of training in the low-variability one. It may be nearly impossible to design an instance-variability-training experiment in which neither confound is present.

Finally, although our central focus in the present research is empirical in nature, we pursued an important formal-modeling goal as well. In particular, we evaluated the extent to which extant formal models of human category learning could capture the overall qualitative pattern of observed results. A classic contrast in the present domain is often made between the predictions from prototype and exemplar models of categorization. According to prototype models, people form summary representations of categories, usually formalized as the central tendency of each category distribution, and classify items on the basis of their similarity to the prototypes of the alternative categories. By contrast, as we described previously, according to exemplar models, people represent categories by storing the individual training exemplars in memory. Our prior hypothesis was that under the present types of training conditions - very large category sizes in which the sets of training exemplars were likely to cover fairly completely the category space - we would indeed observe generalization enhancement for high-distortion test patterns under high-variability training conditions. Furthermore, our prior hypothesis was that extant versions of exemplar models would capture such an empirical pattern of results, because the novel high-distortion test patterns would likely be highly similar to at least some of the high-distortion training instances. To anticipate, our hypotheses were disconfirmed: instead, as observed by Bowman and Zeithamova (2023) and by Homa and Cultice (1984), there was no evidence that high-variability training conditions led to generalization enhancement for the high-distortion test patterns, even under the present conditions in which category training size was very large. Furthermore, to our surprise, application of baseline versions of exemplar models yielded predictions that were in accord with these empirical findings. This in turn forced a re-evaluation of our intuitions for the operation of such models, which we describe in our _General discussion_. In addition, as we discuss in more depth in our _Formal modeling_ section, we believe that the results and modeling investigations will also promote further advancements in theorizing involving prototype models.

## Method

### Subjects

The study was approved by the Indiana University Institutional Review Board. The subjects were 304 students from Indiana University who participated in partial fulfillment of an undergraduate psychology course requirement. Subjects were randomly assigned to one of four training conditions. There were 77 subjects in the low-distortion condition, 78 in the medium-distortion condition, 75 in the high-distortion condition and 74 in the mixed-distortion condition. The sample sizes are sufficient to yield power >.90 to detect a medium-size effect (Cohen's d =.50, \(\alpha\).05) in pairwise tests (Faul et al., 2007). All subjects had normal or corrected-to-normal vision.

### Stimuli and apparatus

The stimuli were dot patterns generated using Posner et al.'s (1967) procedure (i.e., the algorithm used by Posner & Keele, 1968). For each individual subject, prototypes for three different categories were generated by placing nine dots at random grid positions in the central 30 \(\times\) 30 area of a 50 \(\times\) 50 grid. Different training and transfer patterns of each category were generated using the statistical-distortion procedure of Posner et al. (1968). Each pattern was constructed from the prototype of its category by displacing each dot by a random direction and distance in accord with the Posner et al. (1967) procedure. Low-, medium-, and high-level distortions were generated by moving the individual dots, on average, 4, 6 and 7.7 Posner-levels away from their prototype. Each individual subject was presented with a unique set of randomly generated prototypes and training and transfer patterns. In Fig. 1 we show representative examples of the types of patterns produced with this procedure. It is intuitively clear that the degree of similarity to the prototype decreases as the level of distortion increases. In addition, it will generally be the case that two same-category low-distortion patterns chosen at random will be more similar to one another than two same-category medium-distortion patterns chosen at random; and that two same-category medium-distortion patterns chosen at random will be more similar to one another than two same-category high-distortion patterns chosen at random. The patterns chosen for illustration in Fig. 1 are intended to reflect those tendencies.

We used Dell computers to display the stimuli and control the experiment. The patterns were white in color and displayed at the center of a gray computer screen.

### Procedure

A standard learning-transfer paradigm was used. In the learning phase, subjects were trained to classify a set of training patterns into three categories, A, B, and C. On each trial, a dot pattern was presented at the center of the computer screen and remained visible until a subject responded with a key press. After the response, corrective feedback appeared for 2 s below the presented pattern. In all conditions, the learning phase consisted of ten training blocks, each of which had 27 trials (270 trials total). A different set of training patterns was randomly generated in each of the ten training blocks. No individual training item was ever repeated. The learning phase was followed shortly after by a transfer phase where subjects classified selected novel patterns as well as a subset of training patterns into the same three categories. No corrective feedback was given on any test trial.

For each individual participant, three random prototypes were generated to define each of the three categories. Participants were randomly assigned to one of the four training conditions described above. In each condition, 90 training patterns (nine per block) were randomly generated around each of the three category prototypes (270 patterns in total). In all conditions, an equal number of training patterns from each category was presented in each block. In the mixed condition, there was an equal number of low, medium, and high distortions from each category in each block. In all conditions, order of presentation of the generated patterns was randomized within each block.

The test patterns consisted of 27 old patterns that were presented in the training phase (nine per category, with at least two of the 27 patterns from each of the ten training blocks), three prototypes (one per category), nine new low-level distortions (three per category), 18 new medium-level distortions (six per category), and 27 new high-level distortions (nine per category). Each pattern was presented once in a random order for each subject for a total of 84 test trials. We decided to use fewer test trials for prototypes and low distortions than for medium and high distortions for two reasons. First, because all low distortions within a category are highly similar to their prototype as well as to one another, there is the possibility that observers can learn the category structure at the time of test if there are too many presentations of the prototypes and low distortions (e.g., Palmeri & Flanery, 1999; Zaki & Nosofsky, 2007). Second, in the case of larger-size categories, Homa and Vosburgh (1976, Fig. 1) found that the main benefits of higher-distortion training on subsequent generalization were for the medium- and high-distortion test patterns, so those seemed the more important patterns to test.

## Results

### Learning

Figure 2 shows the average proportion of correct classification responses over the training blocks for each of the four training conditions. Across all the training conditions, classification accuracy gradually improves over the course of training. The low-distortion training condition shows the highest accuracy, the medium- and mixed-distortion conditions show intermediate levels of accuracy, and the high-distortion condition shows the lowest accuracy. To confirm these observations, we conducted a 4 \(\times\) 10 mixed-model ANOVA using training

conditions (low, medium, high, mixed) and blocks as factors. The analysis revealed a significant main effect of blocks, F(6.35, 1905.07) = 84.44, MSE = 0.018, p \(<\).001, \(\eta\)2 =.220 (the Greenhouse-Geisser correction was applied for violation of the sphericity assumption). The main effect of training conditions was also significant, F(3,300) = 82.85, MSE = 0.200, p \(<\).001, \(\eta\)2 =.453, as was the interaction effect between learning condition and blocks, F(19.05, 1905.07) = 2.865, MSE = 0.018, p \(<\).001, \(\eta\)2 =.028. To confirm the differences in classification performance across training conditions near the end of the training phase, we compared the mean proportion of correct responses for the last three training blocks. The analysis showed that the mean proportion of correct responses is higher in the low condition (M = 0.885) than in the medium condition, (M = 0.691), t(141.2) = 7.55, p \(<\).001, and higher in the medium condition than in the high condition, (M = 0.499), t(149.4) = 6.92, p \(<\).001.

### Transfer

Figure 3 shows the mean proportion of correct responses for the different types of test patterns as a function of training conditions. The general trend is that, in all conditions, classification accuracy is the highest for the prototypes, and decreases in the order of low-, medium-, and high-level distortion test patterns. In addition, the general trend is that overall transfer performance is best in the low-distortion training condition, intermediate in the medium- and mixed-distortion training conditions, and worst in the high-distortion training condition. Among our key questions was whether higher-distortion training might specifically benefit generalization performance for higher-distortion transfer items, as has been reported in cases in which participants were trained to a common learning criterion across conditions (Homa & Vosburgh, 1976; Posner & Keele, 1968). Inspection of Fig. 3 reveals clearly that such a result was not observed in the present paradigm. Instead, the new high

Figure 3: Mean proportion of correct classifications during the test phase as a function of pattern type and training condition. newlow = new low distortions, newmed = new medium distortions, newhigh = new high distortions

distortions were classified with notably _lower_ accuracy in the high-distortion training condition than in the three other conditions. In addition, the new medium distortions were also classified with _lower_ accuracy in the medium-distortion and mixed-distortion training condition than in the low-distortion condition. Another question of interest, which is potentially relevant to evaluating predictions from formal models, concerns transfer performance on the old-distortion training patterns compared to new patterns of the same level of distortion. Although most dot-pattern category-learning paradigms that use small category sizes show patterns of results in which there is an old-item advantage, this does not appear to be the case in the present paradigm in which individual training instances never repeated and category size is extremely large (see Table 1).

To confirm these observations, we first conducted a 4 x 4 mixed-model ANOVA, using training condition (low, medium, high, mixed) and novel pattern type (prototype, new-low, new-medium, new-high) as factors. The analysis revealed a significant main effect of pattern type, F(2.60, 779.4) = 128.5, MSE = 0.020, p \(<.001\), n2 =.092; a significant main effect of training condition, F(3,300) = 15.4, MSE = 0.146, p \(<.001\), n2 =.091; and a significant interaction between the two factors, F(7.79, 779.4) = 4.46, MSE = 0.020, p \(<.001\), n2 =.010. In more focused tests of interest, we found that for the new high-distortion patterns, the mean proportion of correct responses is significantly lower in the high condition (M =.512) than in the medium condition, (M =.631), t(150.7) = 4.024, p \(<.001\), the mixed condition, (M =.593), t(146.8) = 2.655, p \(=.009\), and the low condition, (M =.637), t(135.5) = 4.786, p \(<.001\). For the new medium-distortion patterns, the mean proportion of correct responses is significantly lower in the medium condition (M =.692) than in the low condition (M =.771), t(146.8) = 2.631, p =.009. We also conducted a 4 x 2 mixed-model ANOVA that compared accuracy across the four conditions on the old and new patterns of the same distortion level. In accord with our earlier observations, this analysis revealed a main effect of training condition, F(3, 300) = 45.4, MSE =.066, p \(<.001\), but no main effect of old-new status of the test item, F(1, 300) = 1.04, MSE =.007, p =.31, and no interaction between the two factors, F(3, 300) = 1.11, MSE =.007, p =.35.

One possibility is that our failure to observe generalization enhancement for high-distortion patterns under higher-variability training may arise because of our random prototype-generation procedure. By chance, for some participants, some of the random prototypes of the alternative categories may have been extremely difficult to discriminate. In such cases, high-variability training may have caused the "noise" to overwhelm the "signal" rather than providing generalization enhancements. We do not have a rigorous theory of dot-pattern similarity at the level of individual items to allow us to evaluate this hypothesis. However, to partially address the issue, we divided the participants of each condition into quartiles based on their overall proportion-correct scores during the test phase and plotted the results separately for each quartile. The results are shown in Fig. 4. Although there are multiple reasons why individual participants may perform poorly in this paradigm, one of the major reasons is that they were tested with difficult-to-discriminate prototype patterns. Thus, we reasoned that participants in the upper quartiles might show generalization enhancement for the high-distortion patterns when provided with high-distortion training, because they likely received easier-to-discriminate prototypes in which the noise would not overwhelm the signal. As can be seen in Fig. 4, however, even for the higher-performing participants, there is no support for the hypothesis that high-distortion training leads to generalization enhancement for the high-distortion test patterns.

## Formal model-based accounts of the classification transfer data

In this section of our article we report preliminary model-based accounts of the patterns of observed classification transfer data, with a focus on the types of exemplar and prototype models that are often contrasted in the dot-pattern prototype-distortion paradigm (e.g., Busemeyer, Dewey, & Medin, 1984; Hintzman, 1986; Homa et al., 1981; Homa, Blair, et al., 2019; Hu & Nosofsky, 2022; Palmer & Flanery, 2002; Shin & Nosofsky, 1992). Our focus here is on the extent to which alternative versions of the models can account for the main qualitative patterns of results in the data. Despite the venerable history of use of the present type of dot-pattern paradigm in investigating the nature of category-learning and representation, we are not aware of any previous studies in which investigators have attempted quantitative fits of formal models to

\begin{table}
\begin{tabular}{l c c} \hline  & \multicolumn{2}{c}{Item type} \\ \cline{2-3} Condition & Old & New \\ \hline Low & 0.860 (0.851) & 0.873 (0.845) \\ Medium & 0.698 (0.689) & 0.692 (0.676) \\ Mixed & 0.697 (0.700) & 0.683 (0.683) \\ High & 0.532 (0.593) & 0.512 (0.562) \\ \hline \end{tabular} _Note._ Values in parentheses are predictions from a baseline version of an exemplar model described in the _Formal models_ section. For the mixed condition, the results are averaged across the patterns with different levels of distortion

\end{table}
Table 1: Mean proportion of correct responses for old and new test items at the same level of distortion from the prototype as a function of training condition individual-subject data at the level of individual patterns in this domain. A major reason for this limitation is that the underlying psychological dimensions of the dot-pattern stimuli are unknown, in large part because the individual dot locations give rise to highly complex emergent configurations. Moreover, in the present case, because each participant was presented with a unique set of patterns defined around randomly generated prototypes, these unknown and complex psychological dimensions will likely be different for each individual observer. Nevertheless, although it is not possible at this stage to engage in such forms of rigorous quantitative model comparison in this domain, it seems reasonable to us to follow previous investigators by testing the extent to which the extant models can capture the broad qualitative patterns of results in the data.

In particular, following Hintzman's (1984, 1986) influential style of modeling performance in the dot-pattern paradigm, we pursue this goal here by using computer-simulation methods intended to produce category structures analogous to those that are thought to be produced through use of the Posner-Keele prototype-distortion procedure. As a representative from the class of exemplar models, rather than using Hintzman's (1986) MINVERVA-2 model, we instead use Hu and Nosofsky's (2022) simulation-based version of Nosofsky's (1986) _generalized context model_ (GCM). As argued by Jamieson et al. (2022), when varieties of formal models from the same general theoretical class (such as exemplar models) yield converging predictions, it reinforces more strongly the general principles that those models are intended to formalize.

### General simulation procedure

Following Hu and Nosofsky (2022), in our main simulations, we represent the dot patterns as points in a six-dimensional psychological space. This is consistent with multidimensional-scaling (MDS) studies by Shin and Nosofsky (1992), who reported that six-dimensional MDS solutions provided good accounts of similarity-ratings data in a study that involved use of a fixed set of prototype-distorted dot-pattern stimuli for all participants. For each simulated subject and for each category, a prototype was generated by randomly choosing a value in the range [0, _between_] along each of the six dimensions. The free parameter _between_ influences the

Figure 4: Mean proportion of correct classifications during the test phase as a function of pattern type and training condition, broken down by overall subject-performance quartiles

degree to which different category prototypes are dissimilar from one another. In general, relatively larger values of _between_ will result in larger distances among category prototypes, hence less similarity between categories.

Next, for each category, we generated a set of statistical distortions (\(x\)) by adding noisy values to the prototype patterns. The greater the distortion level of the to-be-generated patterns (low, medium, high), the greater was the magnitude of these noisy values. Specifically, the simulation procedure sampled random \(z\) scores from a standard normal distribution and added scaled values of \(z\) to the dimension values of the corresponding category prototype (\(P\)). Different scaling factors were used to represent different levels of distortion, as follows:

\[\begin{split} x_{im}=P_{im}+\textit{within}^{*}low^{*}z,\text{ for low distortions}\\ x_{im}=P_{im}+\textit{within}^{*}medium^{*}z,\text{ for medium distortions}\\ x_{im}=P_{im}+\textit{within}*\textit{high}*z,\text{ for high distortions}\end{split} \tag{1}\]

In Eq. 1, \(P_{im}\) denotes the value of Prototype \(i\) on dimension \(m\), and \(x_{im}\) denotes the value of a statistical distortion generated from Prototype \(i\) on dimension \(m\). The parameter _within_ is a freely estimated scaling parameter that primarily determines the relative degree of _within_-category dissimilarity (i.e., how dissimilar the statistical distortions of a category tend to be from their generating prototypes as well as from one another). The parameters _low_, _medium_, and _high_ specify the average magnitude of low, medium, and high distortion levels. To reduce the number of free parameters, in a baseline version of the model we set these values at the average dot-distance movements produced by the Posner-Keele statistical-distortion algorithm,2 reported by Homa et al. (2019) to be _low_ = 1.20, _medium_ = 2.80, and _high_ = 4.60. Thus, the simulation procedure produces noisy values of \(x_{im}\) that tend to lie close to the prototype values \(P_{im}\) in the case of low distortions, a medium distance away in the case of medium distortions, and a large distance away in the case of high distortions.

Footnote 2: Allowing the values of _low_, _medium_, and _high_ to vary instead as free parameters led to relatively small improvements in model fit.

### Exemplar model of categorization

Once the patterns are generated for each individual simulation, the standard equations of the GCM (Nosofsky, 1986, 2011) are used to generate the classification predictions in the transfer phase. According to the GCM, the probability that test pattern \(i\) is classified into Category A is found by summing its similarity to all the training examples \(a\) that belong to Category A (\(s_{ia}\)), and dividing by the summed similarity of \(i\) to all the training examples of all categories:

\[\Pr\left(A|i\right)=\frac{\left(\sum_{a\in A}S_{ia}\right)^{\gamma}}{\left( \sum_{a\in A}S_{ia}\right)^{\gamma}+\left(\sum_{b\in B}S_{ib}\right)^{\gamma}+ \left(\sum_{c\in C}S_{ic}\right)^{\gamma}} \tag{2}\]

where the parameter \(\gamma\) is a response-scaling parameter. When \(\gamma\) = 1, observers respond by probability-matching to the relative summed similarities of each category; as \(\gamma\) grows larger in magnitude, observers respond more deterministically with the category that yields the largest summed similarity (Ashby & Maddox, 1993).

The similarities between patterns are computed as follows. First, for the present types of stimuli, the standard Euclidean distance formula is used to compute the distance between test pattern \(i\) and training example \(j\),

\[d_{ij}=\left[\sum_{m}\left(x_{im}-x_{jm}\right)^{2}\right]^{1/2} \tag{3}\]

where \(x_{im}\) and \(x_{jm}\) denotes the values of patterns i and j on dimension \(m\), respectively. Next, following Shepard (1987), the similarity between test pattern i and training example \(j\) (\(s_{ij}\)) is assumed to be an exponential-decay function of their distance,

\[s_{ij}=e^{-cd_{ij}} \tag{4}\]

where \(c\) is a sensitivity parameter that describes the rate at which similarity declines with distance. The sensitivity parameter provides a measure of overall discriminability among patterns in the feature space.

In the present formalization, it turns out that the parameters _between_, _within_, and \(c\) cannot be estimated separately from one another, one can estimate only their relative values. Without loss of generality, here we set _between_ = 2 for scaling convenience. Thus, the present baseline version of the exemplar model uses only three free parameters: _within_ (Equation 1), \(\gamma\) (Equation 2), and \(c\) (Equation 4).

We fitted the exemplar model to the classification-transfer data of Fig. 3 by searching for the values of the free parameters that minimized the sum of squared deviations between the predicted and observed correct-classification probabilities across all the item types across all the training conditions. In the baseline model, the three free parameters were constrained to be held fixed across the different training conditions. We conducted 10,000 simulations to generate the predictions. For each individual simulated subject, the set of predicted classification probabilities was computed. We then averaged across the predictions from the 10,000 simulated subjects to generate the predictions of the averaged data in Fig. 3. We used the Hook and Jeeves (1961) parameter-search algorithm to locate the values of the best-fitting parameters, and used ten different random starting configurations for the parameter search.

The predictions from the baseline exemplar model are shown in the second panel from the left of Fig. 5. Tofacilitate comparisons, the observed data are displayed again in the first panel. The best-fitting parameters and summary fit are reported in Table 2. Inspection of Fig. 5 reveals that the baseline exemplar model does a good job of capturing the main qualitative trends in the observed data, predicting correctly the main effects of test-pattern type, training condition, and the form of the interaction between these variables. As can be seen, it also predicts extremely small differences in accuracy between the old training patterns and new transfer patterns at the same distortion level, which is the basic pattern seen in the data - for details, see Table 1. The reason is that, in the present paradigm, the perfect similarity-match of an old test item to its own memory representation is swamped by its similarity to the 89 other training items from its category, such that there is relatively little difference in summed similarity for old and new test patterns at the same level of distortion. Finally, in contrast to our prior intuitions, the baseline model predicts correctly that the new high distortions are classified with lower accuracy in the high-distortion training condition than in the low, medium, and mixed training conditions, which was the central issue of interest in this study.

A question that arises is the extent to which the qualitative pattern of predictions for the baseline exemplar model in Fig. 5 is robust across different parameter settings. To explore this question, we generated predictions from the baseline model that varied the settings of the _within_-category scaling parameter and the sensitivity parameter \(c\). We set both parameters at their best-fitting values, at values 50% below the magnitude of their best-fitting values, and at values 50% greater than their best-fitting values. The predictions for each combination of parameter values are shown

\begin{table}
\begin{tabular}{l l l l l} \hline Parameter & Baseline Exemplar & Baseline Prototype & Multiple-Sensitivity & Multiple-Sensitivity \\  & & & Exemplar & Prototype \\ \hline _between_ & (2.000) & (2.000) & (2.000) & (2.000) \\ _within_ & 0.210 & 0.134 & 0.187 & 0.121 \\ \(c\) & 0.475 & 1.285 & — & — \\ \(\tau\) & 5.000 & (1.000) & 4.907 & (1.000) \\ \(c_{Low}\) & — & — & 0.497 & 1.748 \\ \(c_{Med}\) & — & — & 0.445 & 1.260 \\ \(c_{High}\) & — & — & 0.355 & 0.799 \\ \(c_{Minal}\) & — & — & 0.434 & 1.269 \\ _\# Parms_ & 3 & 2 & 6 & 5 \\ _SSD_ & 0.016 & 0.108 & 0.004 & 0.006 \\ \hline \end{tabular} Parameter values in parentheses are held fixed at constrained settings. Parameter values with dashes in cells do not appear for the specific model SSD = sum of squared deviations between observed and predicted classification probabilities, # Parms = number of free parameters used by the model. The upper limit on the parameter \(\gamma\) in the exemplar model was set at 5

\end{table}
Table 2: Model-fitting results for exemplar and prototype models

Figure 5: Observed and predicted proportions of correct classifications during the test phase as a function of pattern type and training condition. The panels, ordered from left to right, provide the following information: 1. Observed data; 2. Exemplar model 1c: baseline exemplar model with the sensitivity parameter (c) held fixed across the four training conditions; 3. Prototype model 1c: baseline prototype

in Fig. 6. As can be seen, although varying the magnitude of these free parameters results in changing the overall predicted levels of accuracy, the overall pattern of qualitative predictions remains the same.

From a quantitative perspective, the baseline model tends to overestimate overall accuracy in the high-distortion training condition and to underestimate overall accuracy in the low-distortion condition. As we report below, by adding certain free parameters to the model, there are approaches to improving the precise quantitative predictions. In our view, however, the important result is that the low-parameter baseline model appears to capture in natural fashion the main qualitative pattern of results observed in the paradigm.

### Prototype model of categorization

It is also of interest to consider the predictions of alternative versions of the types of prototype models that are often fitted to classification data (e.g., Bowman & Zeithamova, 2023; Minda & Smith, 2001; Nosofsky, 1987; Palmeri & Flanery, 2002; Reed, 1972). The prototype of a category is generally formalized as the central tendency of the category, computed across all the category's training instances. Within the present modeling framework, the prototype model is the same as the exemplar model, except that instead of summing similarities of test items to the individual training exemplars, one computes their similarity to the prototypes. Thus, according to the prototype model, the probability that pattern \(i\) is classified into Category A is given by

\[\Pr\left(A|i\right)=\frac{\left(S_{i,P_{A}}\right)^{\gamma}}{\left(S_{i,P_{A}} \right)^{\gamma}+\left(S_{i,P_{B}}\right)^{\gamma}+\left(S_{i,P_{C}}\right)^{ \gamma}} \tag{5}\]

where \(S_{i,P_{A}}\) is the similarity of test pattern \(i\) to the prototype of category A. These similarities are computed in analogous fashion as in the exemplar model through use of Eqs. 3 and 4. Note that within the framework of the prototype model, the response-scaling parameter \(\gamma\) cannot be estimated separately from the sensitivity parameter \(c\) (as defined in Eq. 4),

Figure 6: Predicted proportion of correct classifications from the baseline exemplar model across different combinations of its parameter settings

so \(\gamma\) is fixed at 1 for the prototype model (for extensive discussion, see Nosofsky & Zaki, 2002). Thus, the baseline version of the prototype model uses two free parameters: the _within_-category scaling parameter (Eq. 1) and the sensitivity parameter \(c\) (Eq. 4).

We started by fitting this baseline prototype model to the classification transfer data using the same methods as already described for the exemplar model. For each simulation, we computed _empirical_ prototypes of Categories A, B, and C by averaging across the simulated training distortions and then applying Eqs. 5, 3, and 4 to generate the predictions. The best-fitting predictions from the baseline model that we just described are shown in the middle panel of Fig. 5, with best-fitting parameters and summary fit reported in Table 2. As can be seen, the baseline prototype model captures the typicality gradient within each condition, with the prototypes predicted to be the most accurately classified pattern, followed in order by the low, medium, and high distortions. However, the baseline model fails to predict the effect of training condition on performance, with the predicted performance curves essentially overlapping across the different training conditions. In hindsight, the reason for this prediction is self-evident, but we consider it to be instructive: with the present large-size samples of training instances, the computed central tendency of each category is nearly the same regardless of whether participants are trained with low-variability or high-variability training instances. To our knowledge, this issue has not been addressed explicitly in any previous work that has considered the prior predictions from prototype models of the influence of training-instance variability on generalization performance.

Obviously, we do not conclude that the present data rule out all members of the class of prototype models. First, a prototype theorist might argue that our method of creating simulated pattern types for the present paradigm is flawed. Although we accept this criticism, we note that numerous other methods of coding the patterns are likely to also yield nearly identical computed central tendencies across the low-, medium-, and high-distortion training conditions when training-set size is very large.

More interesting is that one needs to specify the operation of additional psychological mechanisms that operate in concert with prototype representations. For example, it may be that there are psychological limits on the number of training instances that can be averaged to compute the central tendency of each category. A related idea is that prototypes are formed using running averages with either a primacy or recency weighting rather than by giving equal weight to each and every training exemplar. In cases involving these types of limited psychological sample sizes, training with low distortions is more likely to yield empirically computed prototypes that better approximate the objective prototypes used to generate the categories.

Still another possibility is that there is variation in the values of the free parameters across the different training conditions. For example, Hahn et al. (2005) reported an experiment in which participants learned to classify objects varying along two continuous dimensions into two contrasting categories, one with low variability and one with high variability. Although their modeling analyses favored the predictions from exemplar models over prototype models, their more important result was that the high-variability category was associated with a lower setting of the sensitivity parameter (\(c\) in Eq. 4) than was the low-variability category (see also Gorman & Goldstone, 2022). Adapting that idea, in the fourth and fifth panels of Fig. 5 we show predictions from the present versions of the exemplar and prototype models in which a separate sensitivity parameter is estimated for each of the four training conditions (low, medium, high, mixed). The best-fitting parameters and summary fits from these extended versions of the models are reported in Table 2. As can be seen, making allowance for this form of parameter variation dramatically improves the predictions from the prototype model (and also improves the predictions from the exemplar model).

The crucial point is that in considering the influence of training-instance variability on category generalization, the types of psychological mechanisms described above have not been at the forefront of past theorizing involving either prototype or exemplar models. Thus, the present results may encourage further theoretical development and empirical investigation involving both classes of models. One line of future work, for example, might be aimed at discovering whether there are limits on the psychological sample sizes associated with the formation of prototypes. A second line of work might seek converging evidence for the hypothesis that there are changes in psychological sensitivity associated with the use of high-variability versus low-variability training instances.

Finally, it is important to re-emphasize and acknowledge that, for reasons explained above, we have engaged in model-fitting of only aggregate data. In principle, no single subject may show a pattern of performance that looks like the group average. Thus, a crucial aim for future research is to develop techniques that might allow the present types of models to be fitted to individual-item results from individual participants in the dot-pattern prototype-distortion paradigm. Possibly, through application of multidimensional-scaling techniques or use of deep-learning approaches for extracting features of complex, high-dimensional objects, one might derive the actual psychological dimensions along which the present types of dot-pattern stimuli are organized (e.g., Annis et al., 2021; Battleday et al., 2020; Nosofsky et al., 2018; Palmeri & Nosofsky, 2001; Peterson et al., 2018; Sanders & Nosofsky, 2020; Shin & Nosofsky, 1992). In that case, rather than relying on use of the present types of simulation-basedmodeling techniques - which were intended to produce category structures analogous to those produced in the dot-pattern prototype-distortion paradigm - we might engage instead in more rigorous forms of individual-item/individual-participant quantitative modeling used in cases in which the psychological dimensions composing the to-be-classified objects are known.3

Footnote 1: We should note that the procedure of fitting these types of models to individual-participant data does not necessarily address the theoretical question of why separate values of the sensitivity parameter may be associated with the different training-instance variability conditions. If the overall distribution of individual-participant sensitivity parameters is found to differ across the four training conditions, there remains the question of why.

## General discussion

The present results lend considerable generality to findings reported by Bowman and Zeithamova (2023) and Homa and Culic (1984): Under conditions in which amount of training across conditions is held constant in prototype-distortion paradigms, high-variability training does not enhance generalization to novel high-distortion test patterns. If anything, the opposite pattern appears to obtain, with low-variability training yielding an advantage. These results place in focused perspective the commonly held impression that high-variability training is beneficial to category generalization in the prototype-distortion paradigm. This impression appears to be derived from studies in which participants in high-variability training conditions were given more total trials of training than were participants in the low-variability training conditions. Future work needs to carefully specify the conditions under which a high-variability training advantage may emerge.

Our present modeling investigations converge with earlier results reported by Hintzman (1986) in suggesting that exemplar-based models naturally predict the qualitative pattern of results reported here. This finding forces a re-adjustment of our own prior intuitions regarding how such models would behave. Prior to conducting the modeling, our intuition was that exemplar models should predict enhanced generalization to novel high-distortion test items if training occurs with high-distortion training instances, especially in situations in which the number of training instances from each category is large, such as was the case in the present experiment. Our thinking was that under such circumstances, the training instances should cover more completely the region of the similarity space in which the high-distortion test patterns are embedded, thereby allowing for more successful generalization to those patterns. Instead, our adjusted intuition is now as follows. Because the category structures in the prototype-distortion paradigm are _generated_ by distorting the prototypes, an ideal observer would classify novel test patterns in terms of their similarity to the prototypes. Low-distortion training examples are far more similar to the generating prototypes than are high-distortion training examples. Hence, an exemplar-based representation consisting of low-distortion training exemplars would better approximate the ideal-observer prototype representation than would an exemplar-based representation composed of high-distortion training exemplars.

If this adjusted intuition has merit, then it leads us to a new prediction. Here, we limited consideration to cases in which categories were generated through statistical distortion of prototypes and in which all categories had the same overall levels of variability. This is an extremely special case, however, of the possible types of category structures. If observers need to learn categories that are not structured in this manner, then very different forms of category training may be optimal. If categories are not structured in a simple manner around their central tendencies, then training with only low distortions of the central tendencies is likely to be detrimental to successful generalization. Higher-variability training may turn out to be highly beneficial in cases involving more complex category structures.

Indeed, work reported by Wahlheim, Finn, and Jacoby (2012) involving the learning of real-world bird categories, and by Nosofsky, Sanders, Zhu, and McDaniel (2019) involving real-world rock categories, is suggestive along these lines. Rather than manipulating variability per se, these researchers compared conditions involving manipulations of category size. In small-size/high-repetition conditions, observers were trained on small numbers of distinct instances each repeated numerous times, whereas in large-size/low-repetition conditions, observers were trained on large numbers of distinct instances each repeated fewer times. Total number of training trials was equated across conditions. Although performance on the original training exemplars was better in the small-size/high-repetition condition, generalization to novel transfer items was better in the large-size/low-repetition condition. Because increasing category size is likely to increase category "coverage" in a manner analogous to increasing variability of training instances, these results provide suggestive evidence in support of the hypothesis advanced above. Nevertheless, future research involving the learning of real-world categories needs to manipulate the factor of training-instance variability in and of itself to test whether high-variability training may truly be beneficial in such domains.



## References

* (1)
* Annis et al. (2021) Annis, J., Gauthier, I., & Palmeri, T. J. (2021). Combining convolutional neural networks and cognitive models to predict novel object recognition in humans. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _47(5)_, 785.
* Ashby and Maddox (1992) Ashby, F. G., & Maddox, W. T. (1992). Complex decision rules in categorization: Contrasting novice and experienced performance. _Journal of Experimental Psychology: Human Perception and Performance_, _18(1)_, 50.
* Ashby and Maddox (1993) Ashby, F. G., & Maddox, W. T. (1993). Relations between prototype, exemplar, and decision bound models of categorization. _Journal of Mathematical Psychology_, _37_(3), 372-400.
* Battleday et al. (2020) Battleday, R. M., Peterson, J. C., & Griffiths, T. L. (2020). Capturing human categorization of natural images by combining deep networks and cognitive models. _Nature Communications_, _11(1)_, 5418.
* Bowman and Zeithamova (2020) Bowman, C. R., & Zeithamova, D. (2020). Training set coherence and set size effects on concept generalization and recognition. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _46_(8), 1442.
* Bowman and Zeithamova (2023a) Bowman, C. R., & Zeithamova, D. (2023a). Coherent category training enhances generalization in prototype-based categories. _Journal of Experimental Psychology_ Learning, Memory, and Cognition.
* Bowman and Zeithamova (2023b) Bowman, C., & Zeithamova, D. (2023b). High coherence among training exemplars promotes broad generalization of face families. [https://doi.org/10.31234/osfi.io/pm3j](https://doi.org/10.31234/osfi.io/pm3j), [https://psyariv.com/pm3j](https://psyariv.com/pm3j)
* Busemeyer et al. (1984) Busemeyer, J. R., Dewey, G. I., & Medin, D. L. (1984). Evaluation of exemplar-based generalization and the abstraction of categorical information. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _10_(4), 638.
* Cohen et al. (2001) Cohen, A. L., Nosofsky, R. M., & Zaki, S. R. (2001). Category variability, exemplar similarity, and perceptual classification. _Memory & Cognition_, _29_(8), 1165-1175.
* Doyle and Hourian (2016) Doyle, M. E., & Hourian, K. L. (2016). Metacognitive monitoring during category learning: How success affects future behaviour. _Memory_, _24_(9), 1197-1207.
* Faul et al. (2007) Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences. _Behavior Research Methods_, _39_, 175-191.
* Gorman and Goldstone (2022) Gorman, T. E., & Goldstone, R. L. (2022). An instance-based model account of the benefits of varied practice in visuomotor skill. _Cognitive Psychology_, _137_, 101491.
* Hahn et al. (2005) Hahn, U., Bailey, T. M., & Elvin, L. B. (2005). Effects of category diversity on learning, memory, and generalization. _Memory & Cognition_, _33_(2), 289-302.
* Hintzman (1984) Hintzman, D. L. (1984). MINERVA 2: A simulation model of human memory. _Behavior Research Methods_, _Instruments, & Computers_, _16(2)_, 96-101.
* Hintzman (1986) Hintzman, D. L. (1986). "schema abstraction" in a multiple-trace memory model. _Psychological Review_, _93_(4), 411.
* Homa et al. (2019) Homa, D., Blair, M., McClure, S. M., Medema, J., & Stone, G. (2019). Learning concepts when instances never repeat. _Memory & Cognition_, _47_, 395-341.
* Homa and Culic (1984) Homa, D., & Culic, J. C. (1984). Role of feedback, category size, and stimulus distortion on the acquisition and utilization of ill-defined categories. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _10_(1), 83.
* Homa et al. (1981) Homa, D., Sterling, S., & Trepel, L. (1981). Limitations of exemplar-based generalization and the abstraction of categorical information. _Journal of Experimental Psychology: Human Learning and Memory_, _7_(6), 418.
* Homa and Vosburgh (1976) Homa, D., & Vosburgh, R. (1976). Category breadth and the abstraction of prototypical information. _Journal of Experimental Psychology: Human Learning and Memory_, _2_(3), 322.
* Hook and Jeeves (1961) Hook, R., & Jeeves, T. A. (1961). Direct search solution of numerical and statistical problems. _Journal of the Association for Computing Machinery_, _8_(2), 212.
* Hu and Nosofsky (2022) Hu, M., & Nosofsky, R. M. (2022). Exemplar-model account of categorization and recognition when training instances never repeat. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _48_, 1947-1969.
* Jamieson et al. (2022) Jamieson, R. K., Johns, B. T., Vokey, J. R., & Jones, M. N. (2022). Instance theory as a domain-general framework for cognitive psychology. _Nature Reviews Psychology_, _1_(3), 174-183.
* Medin and Schaffer (1978) Medin, D. L., & Schaffer, M. M. (1978). Context theory of classification learning. _Psychological Review_, _85_(3), 207.
* Minda and Smith (2001) Minda, J. P., & Smith, J. D. (2001). Prototypes in category learning: The effects of category size, category structure, and stimulus complexity. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _27_(3), 775.
* Nosofsky (1986) Nosofsky, R. M. (1986). Attention, similarity, and the identification-categorization relationship. _Journal of Experimental Psychology: General_, _115_(1), 39.
* Nosofsky (1987) Nosofsky, R. M. (1987). Attention and learning processes in the identification and categorization of integral stimuli. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _13_(1), 87.
* Nosofsky (2011) Nosofsky, R. M. (2011). The generalized context model: An exemplar model of classification. In E. Pothos and A Wills (Eds.), _Formal approaches in categorization_, 18-39.
* Nosofsky et al. (2018) Nosofsky, R. M., Sanders, C. A., Meagher, B. J., & Douglas, B. J. (2018). Toward the development of a feature-space representation for a complex natural category domain. _Behavior Research Methods_, _50_, 530-556.
* Nosofsky et al. (2019) Nosofsky, R. M., Sanders, C. A., Zhu, X., & McDaniel, M. A. (2019). Model-guided search for optimal natural-science-category training exemplars: A work in progress. _Psychonomic Bulletin & Review_, _26_, 48-76.
* Nosofsky and Zaki (2002) Nosofsky, R. M., & Zaki, S. R. (2002). Exemplar and prototype models revisited: Response strategies, selective attention, and stimulus generalization. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _28_(5), 924.
* Palmeri and Flanery (1999) Palmeri, T. J., & Flanery, M. A. (1999). Learning about categories in the absence of training: Profound amnesia and the relationship between perceptual categorization and recognition memory. _Psychological Science_, _10_(6), 526-530.
* Palmeri and Flanery (2002) Palmeri, T. J., & Flanery, M. A. (2002). Memory systems and perceptual categorization. In _Psychology of learning and motivation_ (Vol. 41, pp. 141-189). Academic Press.
* Palmeri et al. (2019)Palmeri, T. J., & Nosofsky, R. M. (2001). Central tendencies, extreme points, and prototype enhancement effects in ill-defined perceptual categorization. _The Quarterly Journal of Experimental Psychology Section A_, _54_(1), 197-235.
* Peterson et al. (2018) Peterson, J. C., Abbott, J. T., & Griffiths, T. L. (2018). Evaluating (and improving) the correspondence between deep neural networks and human representations. _Cognitive Science_, _42_(8), 2648-2669.
* Peterson et al. (1973) Peterson, M. J., Meagher, R. B., Jr., Chait, H., & Gillie, S. (1973). The abstraction and generalization of dot patterns. _Cognitive Psychology_, _4_(3), 378-398.
* Posner et al. (1967) Posner, M. I., Goldsmith, R., & Welton, K. E., Jr. (1967). Perceived distance and the classification of distorted patterns. _Journal of Experimental Psychology_, _73_(1), 28.
* Posner and Keele (1968) Posner, M. I., & Keele, S. W. (1968). On the genesis of abstract ideas. _Journal of Experimental Psychology_, _77_(3p1), 353.
* Posner and Keele (1970) Posner, M. I., & Keele, S. W. (1970). Retention of abstract ideas. _Journal of Experimental Psychology_, _83_(2p1), 304.
* Raviv et al. (2022) Raviv, L., Lupyan, G., & Green, S. C. (2022). How variability shapes learning and generalization. _Trends in Cognitive Sciences_, _26_(6), 462-483.
* Reed (1972) Reed, S. K. (1972). Pattern recognition and categorization. _Cognitive Psychology_, _3_(3), 382-407.
* Sanders and Nosofsky (2020) Sanders, C. A., & Nosofsky, R. M. (2020). Training deep networks to construct a psychological feature space for a natural-object category domain. _Computational Brain & Behavior_, \(3\), 229-251.
* Shepard (1987) Shepard, R. N. (1987). Toward a universal law of generalization for psychological science. _Science_, _237_(4820), 1317-1323.
* Shin and Nosofsky (1992) Shin, H. J., & Nosofsky, R. M. (1992). Similarity-scaling studies of dot-pattern classification and recognition. _Journal of Experimental Psychology: General_, _121_(3), 278.
* Stewart and Chater (2002) Stewart, N., & Chater, N. (2002). The effect of category variability in perceptual categorization. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, _28_(5), 893.
* Wahlheim et al. (2012) Wahlheim, C. N., Finn, B., & Jacoby, L. L. (2012). Metacognitive judgments of repetition and variability effects in natural concept learning: Evidence for variability neglect. _Memory & Cognition_, _40_, 703-716.
* Zaki and Nosofsky (2007) Zaki, S. R., & Nosofsky, R. M. (2007). A high-distortion enhancement effect in the prototype-learning paradigm: Dramatic effects of category learning during test. _Memory & Cognition_, _35_, 2088-2096.

**Publisher's Note** Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.

Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law.