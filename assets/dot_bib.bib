@article{huExemplarmodelAccountCategorization2022,
  title = {Exemplar-Model Account of Categorization and Recognition When Training Instances Never Repeat},
  author = {Hu, Mingjia and Nosofsky, Robert M.},
  year = {2022},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {48},
  number = {12},
  pages = {1947--1969},
  publisher = {{American Psychological Association}},
  issn = {0278-7393},
  doi = {10.1037/xlm0001008},
  urldate = {2021-03-31},
  abstract = {In a novel version of the classic dot-pattern prototype-distortion paradigm of category learning, Homa et al. (2019) tested a condition in which individual training instances never repeated, and observed results that they claimed severely challenged exemplar models of classification and recognition. Among the results was a dissociation in which participants classified transfer items with high accuracy in the no-repeat condition, yet in old-new recognition tests showed no ability to discriminate between old and new items of the same level of distortion from the prototype. In addition, speed of classification learning was no faster in a condition in which a small set of training instances was repeated continuously compared with the no-repeat condition. Here we show through computer-simulation modeling that exemplar models naturally capture the classification-recognition dissociation in the no-repeat condition, as well as a wide variety of other qualitative effects reported by Homa et al. (2019). We also conduct new conceptual-replication experiments to investigate their reported null effect of repeated versus nonrepeated training instances on speed of classification learning. In contrast to Homa et al. (2019) we find that speed of learning is substantially faster in the repeat condition than in the no-repeat condition, precisely as exemplar models predict. The exemplar model also captures a wide variety of transfer effects observed following the completion of category learning, including the classification-recognition dissociation observed across the repeat and no-repeat conditions. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  keywords = {categorization,Classification (Cognitive Process),Cognitive Processing Speed,computational modeling,Computational Modeling,Dissociation,exemplars,Learning,Models,prototypes,recognition,Recognition (Learning),Training,Transfer (Learning)},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Hu_Nosofsky_2021_Exemplar-model account of categorization and recognition when training.pdf}
}

@article{huHighvariabilityTrainingDoes2024,
  title = {High-Variability Training Does Not Enhance Generalization in the Prototype-Distortion Paradigm},
  author = {Hu, Mingjia and Nosofsky, Robert M.},
  year = {2024},
  month = jan,
  journal = {Memory \& Cognition},
  issn = {1532-5946},
  doi = {10.3758/s13421-023-01516-1},
  urldate = {2024-01-16},
  abstract = {Classic studies of human categorization learning provided evidence that high-variability training in the prototype-distortion paradigm enhances subsequent generalization to novel test patterns from the learned categories. More recent work suggests, however, that when the number of training trials is equated across low-variability and high-variability training conditions, it is low-variability training that yields better generalization performance. Whereas the recent studies used cartoon-animal stimuli varying along binary-valued dimensions, in the present work we return to the use of prototype-distorted dot-pattern stimuli that had been used in the original classic studies. In accord with the recent findings, we observe that high-variability training does not enhance generalization in the dot-pattern prototype-distortion paradigm when the total number of training trials is equated across the conditions, even when training with very large numbers of distinct instances. A baseline version of an exemplar model captures the major qualitative pattern of results in the experiment, as do prototype models that make allowance for changes in parameter settings across the different training conditions. Based on the modeling results, we hypothesize that although high-variability training does not enhance generalization in the prototype-distortion paradigm, it may do so when participants learn more complex category structures.},
  langid = {english},
  keywords = {Categorization,Computational modeling,Generalization,Training-instance variability},
  annotation = {https://osf.io/c6wea/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hu_Nosofsky_2024_High-variability training does not enhance generalization in the.pdf}
}
