{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_items_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m             prototypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate(proto_coords_flattened))\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(test_items), np\u001b[38;5;241m.\u001b[39marray(prototypes)\n\u001b[0;32m---> 33\u001b[0m test_items, prototypes \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# preview\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_items\u001b[38;5;241m.\u001b[39mshape, prototypes\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mprepare_inputs\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     13\u001b[0m prototypes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Your logic here for identifying test items and corresponding prototypes\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_items_df\u001b[49m\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Assuming this retrieves a single test item's coordinates as a flat array\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     test_item_coords \u001b[38;5;241m=\u001b[39m row[coordinate_columns]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Assuming proto_coords is a list of arrays, where each array is the coordinates for a prototype\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Ensure each set of prototype coordinates is flattened before concatenation\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_items_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../dPattern24.csv')  # Update with the path to your dataset\n",
    "\n",
    "# Normalize coordinates\n",
    "coordinate_columns = [f'x{i}' for i in range(1, 10)] + [f'y{i}' for i in range(1, 10)]\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df[coordinate_columns] = scaler.fit_transform(df[coordinate_columns])\n",
    "\n",
    "\n",
    "def prepare_inputs(df):\n",
    "\n",
    "    test_items_df = df[df['Pattern_Token'].isin(['new_low', 'new_med', 'new_high'])]\n",
    "    \n",
    "    prototypes_df = df[df['Pattern_Token'] == 'prototype']\n",
    "    \n",
    "    # Initialize lists to hold the organized inputs\n",
    "    test_items = []\n",
    "    prototypes = []  # This will be a list of lists, each containing three prototypes' coordinates\n",
    "    \n",
    "    # Iterate through test items and find corresponding prototypes\n",
    "    for index, row in test_items_df.iterrows():\n",
    "        subject = row['sbjCode']\n",
    "        category = row['Category']\n",
    "        \n",
    "        # Retrieve the coordinates for the test item\n",
    "        test_item_coords = row[coordinate_columns].values\n",
    "        \n",
    "        # Retrieve prototypes for the subject (assuming one prototype per category)\n",
    "        subject_prototypes = prototypes_df[prototypes_df['sbjCode'] == subject]\n",
    "        \n",
    "        # Placeholder: Retrieve or calculate the exact prototypes for the subject and category\n",
    "        # You need to adapt this logic based on how your prototypes are defined or calculated\n",
    "        proto_coords = [subject_prototypes[subject_prototypes['Category'] == c][coordinate_columns].values\n",
    "                        for c in range(1, 4)]  # Assuming 3 categories\n",
    "        \n",
    "        # Check if we have found all necessary prototypes and the test item\n",
    "        if len(proto_coords) == 3 and all(len(coords) > 0 for coords in proto_coords):\n",
    "            test_items.append(test_item_coords)\n",
    "            prototypes.append(np.concatenate(proto_coords))  # Flatten the prototype coordinates\n",
    "            \n",
    "    return np.array(test_items), np.array(prototypes)\n",
    "\n",
    "\n",
    "test_items, prototypes = prepare_inputs(df)\n",
    "\n",
    "# preview\n",
    "print(test_items.shape, prototypes.shape)\n",
    "print(test_items[0])\n",
    "print(prototypes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 18)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 18)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)        [(None, 18)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 18)]                 0         []                            \n",
      "                                                                                                  \n",
      " model (Functional)          (None, 32)                   12768     ['input_2[0][0]',             \n",
      "                                                                     'input_3[0][0]',             \n",
      "                                                                     'input_4[0][0]',             \n",
      "                                                                     'input_5[0][0]']             \n",
      "                                                                                                  \n",
      " distance_layer (DistanceLa  (None, 3)                    0         ['model[0][0]',               \n",
      " yer)                                                                'model[1][0]',               \n",
      "                                                                     'model[2][0]',               \n",
      "                                                                     'model[3][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12768 (49.88 KB)\n",
      "Trainable params: 12768 (49.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class DistanceLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Assuming inputs is a list of four tensors: [test_item, proto1, proto2, proto3]\n",
    "        test_item, proto1, proto2, proto3 = inputs\n",
    "        \n",
    "        # Compute distances\n",
    "        d1 = tf.norm(test_item - proto1, axis=1)\n",
    "        d2 = tf.norm(test_item - proto2, axis=1)\n",
    "        d3 = tf.norm(test_item - proto3, axis=1)\n",
    "        \n",
    "        # Combine distances into a single tensor\n",
    "        distances = tf.stack([d1, d2, d3], axis=1)\n",
    "        \n",
    "        # Convert distances to probabilities (or any other logic you want)\n",
    "        probabilities = tf.nn.softmax(-distances)  # Inverting distances since lower means more similar\n",
    "        return probabilities\n",
    "\n",
    "# Define the encoder\n",
    "def create_encoder():\n",
    "    inputs = Input(shape=(18,))  # Adjust based on your input shape\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    bottleneck = Dense(32, activation='relu')(x)\n",
    "    return Model(inputs, bottleneck)\n",
    "\n",
    "# Instantiate one encoder model to be shared\n",
    "encoder = create_encoder()\n",
    "\n",
    "# Define model with custom logic\n",
    "test_item_input = Input(shape=(18,))\n",
    "proto1_input = Input(shape=(18,))\n",
    "proto2_input = Input(shape=(18,))\n",
    "proto3_input = Input(shape=(18,))\n",
    "\n",
    "test_item_encoded = encoder(test_item_input)\n",
    "proto1_encoded = encoder(proto1_input)\n",
    "proto2_encoded = encoder(proto2_input)\n",
    "proto3_encoded = encoder(proto3_input)\n",
    "\n",
    "# Use the DistanceLayer to compute the predicted category\n",
    "predicted_category = DistanceLayer()([test_item_encoded, proto1_encoded, proto2_encoded, proto3_encoded])\n",
    "\n",
    "model = Model(inputs=[test_item_input, proto1_input, proto2_input, proto3_input], outputs=predicted_category)\n",
    "\n",
    "# Compile model - the loss function should match your exact needs, possibly custom\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
